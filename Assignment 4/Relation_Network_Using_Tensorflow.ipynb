{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Relation Network Using Tensorflow.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOyAbExXnf5PmiMmBUyXXAw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashaaher/Emerging-Technologies/blob/master/Assignment%204/Relation_Network_Using_Tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Br174NhgeNzI"
      },
      "source": [
        "##Relation Network Using Tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtqRqS7JeDNg"
      },
      "source": [
        "We are considering simple binary classification problem in this collab and see how can we solve them using a relation network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcN0wDveP3g9",
        "outputId": "35a53639-2832-432a-830c-d0662869e5d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install tensorflow==1.12.0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.12.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/cc/ca70b78087015d21c5f3f93694107f34ebccb3be9624385a911d4b52ecef/tensorflow-1.12.0-cp36-cp36m-manylinux1_x86_64.whl (83.1MB)\n",
            "\u001b[K     |████████████████████████████████| 83.1MB 56kB/s \n",
            "\u001b[?25hCollecting keras-applications>=1.0.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12.0) (1.1.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12.0) (1.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12.0) (0.8.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12.0) (1.15.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12.0) (0.35.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12.0) (1.33.2)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12.0) (0.10.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12.0) (3.12.4)\n",
            "Collecting tensorboard<1.13.0,>=1.12.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/53/8d32ce9471c18f8d99028b7cef2e5b39ea8765bd7ef250ca05b490880971/tensorboard-1.12.2-py3-none-any.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 51.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12.0) (1.18.5)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12.0) (0.3.3)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.12.0) (2.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.12.0) (50.3.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow==1.12.0) (3.3.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow==1.12.0) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.13.0,>=1.12.0->tensorflow==1.12.0) (2.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.13.0,>=1.12.0->tensorflow==1.12.0) (3.4.0)\n",
            "Installing collected packages: keras-applications, tensorboard, tensorflow\n",
            "  Found existing installation: tensorboard 2.3.0\n",
            "    Uninstalling tensorboard-2.3.0:\n",
            "      Successfully uninstalled tensorboard-2.3.0\n",
            "  Found existing installation: tensorflow 2.3.0\n",
            "    Uninstalling tensorflow-2.3.0:\n",
            "      Successfully uninstalled tensorflow-2.3.0\n",
            "Successfully installed keras-applications-1.0.8 tensorboard-1.12.2 tensorflow-1.12.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJI8I8O4cv60",
        "outputId": "c187324a-cfa6-4b97-baac-0aaa151c664e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1.12.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhr_t1XjeRyw"
      },
      "source": [
        "Importing Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PE7Udowyc8MR"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBoCzOR3erNR"
      },
      "source": [
        "Randomly generating some 1000 data points for each of these classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wr8cTr2NdDIg"
      },
      "source": [
        "classA = np.random.rand(1000,18)\n",
        "\n",
        "ClassB = np.random.rand(1000,18)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66ZoS1ksey0s"
      },
      "source": [
        "Creating dataset by combining both of these classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnfJ820XdIBP"
      },
      "source": [
        "data = np.vstack([classA, ClassB])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zC89UlDGe4dx"
      },
      "source": [
        "\n",
        "Setting the label,assigning label 1 for classA and label 0 for classB."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Rmuak1ZdKOf"
      },
      "source": [
        "label = np.vstack([np.ones((len(classA),1)),np.zeros((len(ClassB),1))])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZS6PXbZhdNI3",
        "outputId": "18594b7d-8e09-4055-b087-a1c7e6320b8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "data.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000, 18)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yaG2c3qmdPEe",
        "outputId": "0ccab48a-3bed-4b16-f059-8ece46457c13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "label.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dawWenljfDMb"
      },
      "source": [
        "Placeholders for  support set xi and query set xj"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4thmF6DdRje"
      },
      "source": [
        "xi = tf.placeholder(tf.float32, [None, 9])\n",
        "xj = tf.placeholder(tf.float32, [None, 9])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SV2R4QJEfM65"
      },
      "source": [
        "Defining the placeholder for label y"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OA_YqAfbdTw2"
      },
      "source": [
        "y = tf.placeholder(tf.float32, [None, 1])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ce0zBJ1wfVVp"
      },
      "source": [
        "Embedding function to learn the embedding of support set and query set. Using a normal feedforward network as our embedding function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rT6c4jeQdWeO"
      },
      "source": [
        "def embedding_function(x):\n",
        "    \n",
        "    weights = tf.Variable(tf.truncated_normal([9,1]))\n",
        "    bias = tf.Variable(tf.truncated_normal([1]))\n",
        "    \n",
        "    a = (tf.nn.xw_plus_b(x,weights,bias))\n",
        "    embeddings = tf.nn.relu(a)\n",
        "    \n",
        "    return embeddings"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd7TH3CyfkIn"
      },
      "source": [
        "\n",
        "Computing the embeddings of support set xi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkSN_oqldZaV"
      },
      "source": [
        "f_xi = embedding_function(xi)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXSjrp0nfoLa"
      },
      "source": [
        "\n",
        "Computing the embeddings of query set "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWt2-E2Vdbf9"
      },
      "source": [
        "f_xj = embedding_function(xj)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGVETDkHfwzB"
      },
      "source": [
        "Combining both the support set and query set feature vectors "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrPdnTQKddtN"
      },
      "source": [
        "Z = tf.concat([f_xi,f_xj],axis=1)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtwbuuBxf4Ac"
      },
      "source": [
        "defining  relation function as three layered neural network with relu activations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqgHYbGddgG8"
      },
      "source": [
        "def relation_function(x):\n",
        "    w1 = tf.Variable(tf.truncated_normal([2,3]))\n",
        "    b1 = tf.Variable(tf.truncated_normal([3]))\n",
        "    \n",
        "    w2 = tf.Variable(tf.truncated_normal([3,5]))\n",
        "    b2 = tf.Variable(tf.truncated_normal([5]))\n",
        "    \n",
        "    w3 = tf.Variable(tf.truncated_normal([5,1]))\n",
        "    b3 = tf.Variable(tf.truncated_normal([1]))\n",
        "    \n",
        "    #layer1\n",
        "    z1 = (tf.nn.xw_plus_b(x,w1,b1))\n",
        "    a1 = tf.nn.relu(z1)\n",
        "    \n",
        "    #layer2\n",
        "    z2 = tf.nn.xw_plus_b(a1,w2,b2)\n",
        "    a2 = tf.nn.relu(z2)\n",
        "    \n",
        "    #layer3\n",
        "    z3 = tf.nn.xw_plus_b(z2,w3,b3)\n",
        "\n",
        "    #output\n",
        "    y = tf.nn.sigmoid(z3)\n",
        "    \n",
        "    return y"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCr9EmM3gDWB"
      },
      "source": [
        "Passing the concatenated feature vectors of support set and query set to the relation function and get the relation scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POSpyPHadkLs"
      },
      "source": [
        "relation_scores = relation_function(Z)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBzm_jHKgHn1"
      },
      "source": [
        "Defining loss function as mean squared error i.e squared difference between relation scores and actual y value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpE7IS4VdnFz"
      },
      "source": [
        "loss_function = tf.reduce_mean(tf.squared_difference(relation_scores,y))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvSArGUzgip_"
      },
      "source": [
        "Minimizing the loss using adam optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJkogc_ldpwr"
      },
      "source": [
        "optimizer = tf.train.AdamOptimizer(0.1)\n",
        "train = optimizer.minimize(loss_function)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOtPUhPGgqGK"
      },
      "source": [
        "Starting tensorflow session"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-F9S7l7dsb7"
      },
      "source": [
        "sess = tf.InteractiveSession()\n",
        "sess.run(tf.global_variables_initializer())"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aO5S4FqLgy_k"
      },
      "source": [
        "Randomly sample data points for  support set xi and query set xj and train the network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92FrvJGUdu-z",
        "outputId": "4c2d0f80-f841-411c-c5c4-a1693776efd1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for episode in range(1000):\n",
        "    _, loss_value = sess.run([train, loss_function], \n",
        "                             feed_dict={xi:data[:,0:9]+np.random.randn(*np.shape(data[:,0:9]))*0.05,\n",
        "                                        xj:data[:,9:]+np.random.randn(*np.shape(data[:,9:]))*0.05,\n",
        "                                        y:label})\n",
        "    if episode % 100 == 0:\n",
        "        print(\"Episode {}: loss {:.3f} \".format(episode, loss_value))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode 0: loss 0.362 \n",
            "Episode 100: loss 0.250 \n",
            "Episode 200: loss 0.250 \n",
            "Episode 300: loss 0.250 \n",
            "Episode 400: loss 0.250 \n",
            "Episode 500: loss 0.250 \n",
            "Episode 600: loss 0.250 \n",
            "Episode 700: loss 0.250 \n",
            "Episode 800: loss 0.250 \n",
            "Episode 900: loss 0.250 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FHqQyeohCL-"
      },
      "source": [
        "Reference: https://github.com/sudharsan13296/Hands-On-Meta-Learning-With-Python/blob/master/04.%20Relation%20and%20Matching%20Networks%20Using%20Tensorflow/4.5%20Building%20Relation%20Network%20Using%20Tensorflow.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDv8At9GhEnk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}