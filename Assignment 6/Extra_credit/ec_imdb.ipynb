{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ec_imdb.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM2edxCUrz43YJkPNONVBdN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashaaher/Emerging-Technologies/blob/master/Assignment%206/Extra_credit/ec_imdb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2_S-RsUqo0u"
      },
      "source": [
        "try:\n",
        "  import colab\n",
        "  !pip install --upgrade pip\n",
        "except:\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZebsVQdFqyGi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bbbb3b5-768f-4c35-c859-e0e830329674"
      },
      "source": [
        "pip install -q -U --use-feature=2020-resolver tfx"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: --use-feature=2020-resolver no longer has any effect, since it is now the default dependency resolver in pip. This will become an error in pip 21.0.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPKIbW1Jq3Hh"
      },
      "source": [
        "import os\n",
        "import pprint\n",
        "import tempfile\n",
        "import urllib\n",
        "\n",
        "import absl\n",
        "import tensorflow as tf\n",
        "import tensorflow_model_analysis as tfma\n",
        "tf.get_logger().propagate = False\n",
        "pp = pprint.PrettyPrinter()\n",
        "\n",
        "import tfx\n",
        "from tfx.components import CsvExampleGen\n",
        "from tfx.components import Evaluator\n",
        "from tfx.components import ExampleValidator\n",
        "from tfx.components import Pusher\n",
        "from tfx.components import ResolverNode\n",
        "from tfx.components import SchemaGen\n",
        "from tfx.components import StatisticsGen\n",
        "from tfx.components import Trainer\n",
        "from tfx.components import Transform\n",
        "from tfx.components.base import executor_spec\n",
        "from tfx.components.trainer.executor import GenericExecutor\n",
        "from tfx.dsl.experimental import latest_blessed_model_resolver\n",
        "from tfx.orchestration import metadata\n",
        "from tfx.orchestration import pipeline\n",
        "from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n",
        "from tfx.proto import pusher_pb2\n",
        "from tfx.proto import trainer_pb2\n",
        "from tfx.types import Channel\n",
        "from tfx.types.standard_artifacts import Model\n",
        "from tfx.types.standard_artifacts import ModelBlessing\n",
        "from tfx.utils.dsl_utils import external_input\n",
        "\n",
        "\n",
        "%load_ext tfx.orchestration.experimental.interactive.notebook_extensions.skip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TB_3biWXq9VJ"
      },
      "source": [
        "print('TensorFlow version: {}'.format(tf.__version__))\n",
        "print('TFX version: {}'.format(tfx.__version__))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvYd6HesrNDX"
      },
      "source": [
        "train_set, eval_set = tfds.load(\n",
        "    \"imdb_reviews:1.0.0\",\n",
        "    split=[\"train[:10000]+unsupervised[:10000]\", \"test[:10000]\"],\n",
        "    shuffle_files=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIsy7loarVQu"
      },
      "source": [
        "def _dict_to_example(instance):\n",
        "  \"\"\"Decoded CSV to tf example.\"\"\"\n",
        "  feature = {}\n",
        "  for key, value in instance.items():\n",
        "    if value is None:\n",
        "      feature[key] = tf.train.Feature()\n",
        "    elif value.dtype == np.integer:\n",
        "      feature[key] = tf.train.Feature(\n",
        "          int64_list=tf.train.Int64List(value=value.tolist()))\n",
        "    elif value.dtype == np.float32:\n",
        "      feature[key] = tf.train.Feature(\n",
        "          float_list=tf.train.FloatList(value=value.tolist()))\n",
        "    else:\n",
        "      feature[key] = tf.train.Feature(\n",
        "          bytes_list=tf.train.BytesList(value=value.tolist()))\n",
        "  return tf.train.Example(features=tf.train.Features(feature=feature))\n",
        "\n",
        "\n",
        "examples_path = tempfile.mkdtemp(prefix=\"tfx-data\")\n",
        "train_path = os.path.join(examples_path, \"train.tfrecord\")\n",
        "eval_path = os.path.join(examples_path, \"eval.tfrecord\")\n",
        "\n",
        "for path, dataset in [(train_path, train_set), (eval_path, eval_set)]:\n",
        "  with tf.io.TFRecordWriter(path) as writer:\n",
        "    for example in dataset:\n",
        "      writer.write(\n",
        "          _dict_to_example({\n",
        "              \"label\": np.array([example[\"label\"].numpy()]),\n",
        "              \"text\": np.array([example[\"text\"].numpy()]),\n",
        "          }).SerializeToString())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EID_0ExiCHli"
      },
      "source": [
        "context = InteractiveContext()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkA2dn4bCIS8"
      },
      "source": [
        "input_data = external_input(examples_path)\n",
        "\n",
        "input_config = example_gen_pb2.Input(splits=[\n",
        "    example_gen_pb2.Input.Split(name='train', pattern='train.tfrecord'),\n",
        "    example_gen_pb2.Input.Split(name='eval', pattern='eval.tfrecord')\n",
        "])\n",
        "\n",
        "example_gen = ImportExampleGen(input=input_data, input_config=input_config)\n",
        "\n",
        "context.run(example_gen, enable_cache=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jITlKqV7CL1O"
      },
      "source": [
        "for artifact in example_gen.outputs['examples'].get():\n",
        "  print(artifact)\n",
        "\n",
        "print('\\nexample_gen.outputs is a {}'.format(type(example_gen.outputs)))\n",
        "print(example_gen.outputs)\n",
        "\n",
        "print(example_gen.outputs['examples'].get()[0].split_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AeveqSSWCPx2"
      },
      "source": [
        "def make_example_with_unique_id(example, id_feature_name):\n",
        "  \n",
        "  result = tf.train.Example()\n",
        "  result.CopyFrom(example)\n",
        "  unique_id = uuid.uuid4()\n",
        "  result.features.feature.get_or_create(\n",
        "      id_feature_name).bytes_list.MergeFrom(\n",
        "          tf.train.BytesList(value=[str(unique_id).encode('utf-8')]))\n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7k1SExoMCU46"
      },
      "source": [
        "@component\n",
        "def IdentifyExamples(orig_examples: InputArtifact[Examples],\n",
        "                     identified_examples: OutputArtifact[Examples],\n",
        "                     id_feature_name: Parameter[str],\n",
        "                     component_name: Parameter[str]) -> None:\n",
        "\n",
        "  splits_list = artifact_utils.decode_split_names(\n",
        "      split_names=orig_examples.split_names)\n",
        "\n",
        "  for split in splits_list:\n",
        "    input_dir = os.path.join(orig_examples.uri, split)\n",
        "    output_dir = os.path.join(identified_examples.uri, split)\n",
        "    os.mkdir(output_dir)\n",
        "    with beam.Pipeline() as pipeline:\n",
        "      (pipeline\n",
        "       | 'ReadExamples' >> beam.io.ReadFromTFRecord(\n",
        "           os.path.join(input_dir, '*'),\n",
        "           coder=beam.coders.coders.ProtoCoder(tf.train.Example))\n",
        "       | 'AddUniqueId' >> beam.Map(make_example_with_unique_id, id_feature_name)\n",
        "       | 'WriteIdentifiedExamples' >> beam.io.WriteToTFRecord(\n",
        "           file_path_prefix=os.path.join(output_dir, 'data_tfrecord'),\n",
        "           coder=beam.coders.coders.ProtoCoder(tf.train.Example),\n",
        "           file_name_suffix='.gz'))\n",
        "\n",
        "  \n",
        "  identified_examples.split_names = artifact_utils.encode_split_names(\n",
        "      splits=splits_list)\n",
        "  identified_examples.set_string_custom_property(\n",
        "      \"payload_format\",\n",
        "      orig_examples.get_string_custom_property(\"payload_format\"))\n",
        "\n",
        "  return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XX10BRWhCZJI"
      },
      "source": [
        "identify_examples = IdentifyExamples(\n",
        "    orig_examples=example_gen.outputs['examples'],\n",
        "    component_name=u'IdentifyExamples',\n",
        "    id_feature_name=u'id')\n",
        "context.run(identify_examples, enable_cache=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hd2uDKoYCcW3"
      },
      "source": [
        "schema_gen = SchemaGen(statistics=statistics_gen.outputs['statistics'])\n",
        "context.run(schema_gen, enable_cache=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JJ6FTv9Cg0q"
      },
      "source": [
        "train_uri = schema_gen.outputs['schema'].get()[0].uri\n",
        "schema_filename = os.path.join(train_uri, 'schema.pbtxt')\n",
        "schema = tfx.utils.io_utils.parse_pbtxt_file(\n",
        "    file_name=schema_filename, message=schema_pb2.Schema())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcVCfKW4CkqE"
      },
      "source": [
        "validate_stats = ExampleValidator(\n",
        "    statistics=statistics_gen.outputs['statistics'],\n",
        "    schema=schema_gen.outputs['schema'])\n",
        "context.run(validate_stats, enable_cache=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VL1NcaVTCniN"
      },
      "source": [
        "\n",
        "swivel_url = 'https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1'\n",
        "hub_layer = hub.KerasLayer(swivel_url, input_shape=[], dtype=tf.string)\n",
        "\n",
        "\n",
        "def _bytes_feature(value):\n",
        "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))\n",
        "\n",
        "\n",
        "def _float_feature(value):\n",
        "  return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
        "\n",
        "\n",
        "def create_embedding_example(example):\n",
        "  sentence_embedding = hub_layer(tf.sparse.to_dense(example['text']))\n",
        "\n",
        "  sentence_embedding = tf.reshape(sentence_embedding, shape=[-1])\n",
        "\n",
        "  feature_dict = {\n",
        "      'id': _bytes_feature(tf.sparse.to_dense(example['id']).numpy()),\n",
        "      'embedding': _float_feature(sentence_embedding.numpy().tolist())\n",
        "  }\n",
        "\n",
        "  return tf.train.Example(features=tf.train.Features(feature=feature_dict))\n",
        "\n",
        "\n",
        "def create_dataset(uri):\n",
        "  tfrecord_filenames = [os.path.join(uri, name) for name in os.listdir(uri)]\n",
        "  return tf.data.TFRecordDataset(tfrecord_filenames, compression_type='GZIP')\n",
        "\n",
        "\n",
        "def create_embeddings(train_path, output_path):\n",
        "  dataset = create_dataset(train_path)\n",
        "  embeddings_path = os.path.join(output_path, 'embeddings.tfr')\n",
        "\n",
        "  feature_map = {\n",
        "      'label': tf.io.FixedLenFeature([], tf.int64),\n",
        "      'id': tf.io.VarLenFeature(tf.string),\n",
        "      'text': tf.io.VarLenFeature(tf.string)\n",
        "  }\n",
        "\n",
        "  with tf.io.TFRecordWriter(embeddings_path) as writer:\n",
        "    for tfrecord in dataset:\n",
        "      tensor_dict = tf.io.parse_single_example(tfrecord, feature_map)\n",
        "      embedding_example = create_embedding_example(tensor_dict)\n",
        "      writer.write(embedding_example.SerializeToString())\n",
        "\n",
        "\n",
        "def build_graph(output_path, similarity_threshold):\n",
        "  embeddings_path = os.path.join(output_path, 'embeddings.tfr')\n",
        "  graph_path = os.path.join(output_path, 'graph.tfv')\n",
        "  nsl.tools.build_graph([embeddings_path], graph_path, similarity_threshold)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Re-AXtjvCzAL"
      },
      "source": [
        "class SynthesizedGraph(tfx.types.artifact.Artifact):\n",
        "  TYPE_NAME = 'SynthesizedGraphPath'\n",
        "  PROPERTIES = {\n",
        "      'span': standard_artifacts.SPAN_PROPERTY,\n",
        "      'split_names': standard_artifacts.SPLIT_NAMES_PROPERTY,\n",
        "  }\n",
        "\n",
        "\n",
        "@component\n",
        "def SynthesizeGraph(identified_examples: InputArtifact[Examples],\n",
        "                    synthesized_graph: OutputArtifact[SynthesizedGraph],\n",
        "                    similarity_threshold: Parameter[float],\n",
        "                    component_name: Parameter[str]) -> None:\n",
        "\n",
        "  splits_list = artifact_utils.decode_split_names(\n",
        "      split_names=identified_examples.split_names)\n",
        "\n",
        "  train_input_examples_uri = os.path.join(identified_examples.uri, 'train')\n",
        "  output_graph_uri = os.path.join(synthesized_graph.uri, 'train')\n",
        "  os.mkdir(output_graph_uri)\n",
        "\n",
        "  print('Creating embeddings...')\n",
        "  create_embeddings(train_input_examples_uri, output_graph_uri)\n",
        "\n",
        "  print('Synthesizing graph...')\n",
        "  build_graph(output_graph_uri, similarity_threshold)\n",
        "\n",
        "  synthesized_graph.split_names = artifact_utils.encode_split_names(\n",
        "      splits=['train'])\n",
        "\n",
        "  return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-iIkPmOCzrv"
      },
      "source": [
        "synthesize_graph = SynthesizeGraph(\n",
        "    identified_examples=identify_examples.outputs['identified_examples'],\n",
        "    component_name=u'SynthesizeGraph',\n",
        "    similarity_threshold=0.99)\n",
        "context.run(synthesize_graph, enable_cache=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvZBCcnfC7im"
      },
      "source": [
        "train_uri = synthesize_graph.outputs[\"synthesized_graph\"].get()[0].uri\n",
        "os.listdir(train_uri)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCOuZkBsC8Ht"
      },
      "source": [
        "graph_path = os.path.join(train_uri, \"train\", \"graph.tfv\")\n",
        "print(\"node 1\\t\\t\\t\\t\\tnode 2\\t\\t\\t\\t\\tsimilarity\")\n",
        "!head {graph_path}\n",
        "print(\"...\")\n",
        "!tail {graph_path}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CB8rQABeDEr_"
      },
      "source": [
        "\n",
        "!wc -l {graph_path}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ff6Lrw8yDKIE"
      },
      "source": [
        "_transform_module_file = 'imdb_transform.py'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JD3onY1QDOXY"
      },
      "source": [
        "%%writefile {_transform_module_file}\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_transform as tft\n",
        "\n",
        "SEQUENCE_LENGTH = 100\n",
        "VOCAB_SIZE = 10000\n",
        "OOV_SIZE = 100\n",
        "\n",
        "def tokenize_reviews(reviews, sequence_length=SEQUENCE_LENGTH):\n",
        "  reviews = tf.strings.lower(reviews)\n",
        "  reviews = tf.strings.regex_replace(reviews, r\" '| '|^'|'$\", \" \")\n",
        "  reviews = tf.strings.regex_replace(reviews, \"[^a-z' ]\", \" \")\n",
        "  tokens = tf.strings.split(reviews)[:, :sequence_length]\n",
        "  start_tokens = tf.fill([tf.shape(reviews)[0], 1], \"<START>\")\n",
        "  end_tokens = tf.fill([tf.shape(reviews)[0], 1], \"<END>\")\n",
        "  tokens = tf.concat([start_tokens, tokens, end_tokens], axis=1)\n",
        "  tokens = tokens[:, :sequence_length]\n",
        "  tokens = tokens.to_tensor(default_value=\"<PAD>\")\n",
        "  pad = sequence_length - tf.shape(tokens)[1]\n",
        "  tokens = tf.pad(tokens, [[0, 0], [0, pad]], constant_values=\"<PAD>\")\n",
        "  return tf.reshape(tokens, [-1, sequence_length])\n",
        "\n",
        "def preprocessing_fn(inputs):\n",
        " \n",
        "  outputs = {}\n",
        "  outputs[\"id\"] = inputs[\"id\"]\n",
        "  tokens = tokenize_reviews(_fill_in_missing(inputs[\"text\"], ''))\n",
        "  outputs[\"text_xf\"] = tft.compute_and_apply_vocabulary(\n",
        "      tokens,\n",
        "      top_k=VOCAB_SIZE,\n",
        "      num_oov_buckets=OOV_SIZE)\n",
        "  outputs[\"label_xf\"] = _fill_in_missing(inputs[\"label\"], -1)\n",
        "  return outputs\n",
        "\n",
        "def _fill_in_missing(x, default_value):\n",
        " \n",
        "  return tf.squeeze(\n",
        "      tf.sparse.to_dense(\n",
        "          tf.SparseTensor(x.indices, x.values, [x.dense_shape[0], 1]),\n",
        "          default_value),\n",
        "      axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgRlOM_8DTDl"
      },
      "source": [
        "transform = Transform(\n",
        "    examples=identify_examples.outputs['identified_examples'],\n",
        "    schema=schema_gen.outputs['schema'],\n",
        "    transformed_examples=channel.Channel(\n",
        "        type=standard_artifacts.Examples,\n",
        "        artifacts=[standard_artifacts.Examples()]),\n",
        "    module_file=_transform_module_file)\n",
        "context.run(transform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T06v2L22DWcT"
      },
      "source": [
        "transform.outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_JjwhWEDa3C"
      },
      "source": [
        "train_uri = transform.outputs['transform_graph'].get()[0].uri\n",
        "os.listdir(train_uri)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHcFP6HdDeKB"
      },
      "source": [
        "pprint_examples(transform.outputs['transformed_examples'].get()[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2BlfZxsDoXF"
      },
      "source": [
        "def split_train_and_unsup(input_uri):\n",
        "\n",
        "  tmp_dir = tempfile.mkdtemp(prefix='tfx-data')\n",
        "  tfrecord_filenames = [\n",
        "      os.path.join(input_uri, filename) for filename in os.listdir(input_uri)\n",
        "  ]\n",
        "  train_path = os.path.join(tmp_dir, 'train.tfrecord')\n",
        "  unsup_path = os.path.join(tmp_dir, 'unsup.tfrecord')\n",
        "  with tf.io.TFRecordWriter(train_path) as train_writer, \\\n",
        "       tf.io.TFRecordWriter(unsup_path) as unsup_writer:\n",
        "    for tfrecord in tf.data.TFRecordDataset(\n",
        "        tfrecord_filenames, compression_type='GZIP'):\n",
        "      example = tf.train.Example()\n",
        "      example.ParseFromString(tfrecord.numpy())\n",
        "      if ('label_xf' not in example.features.feature or\n",
        "          example.features.feature['label_xf'].int64_list.value[0] == -1):\n",
        "        writer = unsup_writer\n",
        "      else:\n",
        "        writer = train_writer\n",
        "      writer.write(tfrecord.numpy())\n",
        "  return train_path, unsup_path\n",
        "\n",
        "\n",
        "def gzip(filepath):\n",
        "  with open(filepath, 'rb') as f_in:\n",
        "    with gzip_lib.open(filepath + '.gz', 'wb') as f_out:\n",
        "      shutil.copyfileobj(f_in, f_out)\n",
        "  os.remove(filepath)\n",
        "\n",
        "\n",
        "def copy_tfrecords(input_uri, output_uri):\n",
        "  for filename in os.listdir(input_uri):\n",
        "    input_filename = os.path.join(input_uri, filename)\n",
        "    output_filename = os.path.join(output_uri, filename)\n",
        "    shutil.copyfile(input_filename, output_filename)\n",
        "\n",
        "\n",
        "@component\n",
        "def GraphAugmentation(identified_examples: InputArtifact[Examples],\n",
        "                      synthesized_graph: InputArtifact[SynthesizedGraph],\n",
        "                      augmented_examples: OutputArtifact[Examples],\n",
        "                      num_neighbors: Parameter[int],\n",
        "                      component_name: Parameter[str]) -> None:\n",
        "\n",
        "  splits_list = artifact_utils.decode_split_names(\n",
        "      split_names=identified_examples.split_names)\n",
        "\n",
        "  train_input_uri = os.path.join(identified_examples.uri, 'train')\n",
        "  eval_input_uri = os.path.join(identified_examples.uri, 'eval')\n",
        "  train_graph_uri = os.path.join(synthesized_graph.uri, 'train')\n",
        "  train_output_uri = os.path.join(augmented_examples.uri, 'train')\n",
        "  eval_output_uri = os.path.join(augmented_examples.uri, 'eval')\n",
        "\n",
        "  os.mkdir(train_output_uri)\n",
        "  os.mkdir(eval_output_uri)\n",
        "\n",
        "  train_path, unsup_path = split_train_and_unsup(train_input_uri)\n",
        "\n",
        "  output_path = os.path.join(train_output_uri, 'nsl_train_data.tfr')\n",
        "  pack_nbrs_args = dict(\n",
        "      labeled_examples_path=train_path,\n",
        "      unlabeled_examples_path=unsup_path,\n",
        "      graph_path=os.path.join(train_graph_uri, 'graph.tfv'),\n",
        "      output_training_data_path=output_path,\n",
        "      add_undirected_edges=True,\n",
        "      max_nbrs=num_neighbors)\n",
        "  print('nsl.tools.pack_nbrs arguments:', pack_nbrs_args)\n",
        "  nsl.tools.pack_nbrs(**pack_nbrs_args)\n",
        "\n",
        "  gzip(output_path)\n",
        "\n",
        "  copy_tfrecords(eval_input_uri, eval_output_uri)\n",
        "\n",
        "  augmented_examples.split_names = identified_examples.split_names\n",
        "\n",
        "  return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5Z-yYEQDpJu"
      },
      "source": [
        "graph_augmentation = GraphAugmentation(\n",
        "    identified_examples=transform.outputs['transformed_examples'],\n",
        "    synthesized_graph=synthesize_graph.outputs['synthesized_graph'],\n",
        "    component_name=u'GraphAugmentation',\n",
        "    num_neighbors=3)\n",
        "context.run(graph_augmentation, enable_cache=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaZhUgXADsLy"
      },
      "source": [
        "pprint_examples(graph_augmentation.outputs['augmented_examples'].get()[0], 6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWbcLQ7cDx53"
      },
      "source": [
        "\n",
        "_trainer_module_file = 'imdb_trainer.py'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4DkFoDtD6mP"
      },
      "source": [
        "%%writefile {_trainer_module_file}\n",
        "\n",
        "import neural_structured_learning as nsl\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_model_analysis as tfma\n",
        "import tensorflow_transform as tft\n",
        "from tensorflow_transform.tf_metadata import schema_utils\n",
        "\n",
        "\n",
        "NBR_FEATURE_PREFIX = 'NL_nbr_'\n",
        "NBR_WEIGHT_SUFFIX = '_weight'\n",
        "LABEL_KEY = 'label'\n",
        "ID_FEATURE_KEY = 'id'\n",
        "\n",
        "def _transformed_name(key):\n",
        "  return key + '_xf'\n",
        "\n",
        "\n",
        "def _transformed_names(keys):\n",
        "  return [_transformed_name(key) for key in keys]\n",
        "\n",
        "\n",
        "\n",
        "class HParams(object):\n",
        "  \"\"\"Hyperparameters used for training.\"\"\"\n",
        "  def __init__(self):\n",
        "   \n",
        "    self.max_seq_length = 100\n",
        "    self.vocab_size = 10000\n",
        "    self.oov_size = 100\n",
        "    self.distance_type = nsl.configs.DistanceType.L2\n",
        "    self.graph_regularization_multiplier = 0.1\n",
        "   \n",
        "    self.num_neighbors = 1\n",
        "    self.num_embedding_dims = 16\n",
        "    self.num_fc_units = 64\n",
        "\n",
        "HPARAMS = HParams()\n",
        "\n",
        "\n",
        "def optimizer_fn():\n",
        "  return tf.compat.v1.train.RMSPropOptimizer(\n",
        "    learning_rate=0.0001, decay=1e-6)\n",
        "\n",
        "\n",
        "def build_train_op(loss, global_step):\n",
        "  with tf.name_scope('train'):\n",
        "    optimizer = optimizer_fn()\n",
        "    train_op = optimizer.minimize(loss=loss, global_step=global_step)\n",
        "  return train_op\n",
        "\n",
        "\n",
        "\n",
        "def feed_forward_model(features, is_training, reuse=tf.compat.v1.AUTO_REUSE):\n",
        "  \n",
        "\n",
        "  with tf.compat.v1.variable_scope('ff', reuse=reuse):\n",
        "    inputs = features[_transformed_name('text')]\n",
        "    embeddings = tf.compat.v1.get_variable(\n",
        "        'embeddings',\n",
        "        shape=[\n",
        "            HPARAMS.vocab_size + HPARAMS.oov_size, HPARAMS.num_embedding_dims\n",
        "        ])\n",
        "    embedding_layer = tf.nn.embedding_lookup(embeddings, inputs)\n",
        "\n",
        "    pooling_layer = tf.compat.v1.layers.AveragePooling1D(\n",
        "        pool_size=HPARAMS.max_seq_length, strides=HPARAMS.max_seq_length)(\n",
        "            embedding_layer)\n",
        "    pooling_layer = tf.reshape(pooling_layer, [-1, HPARAMS.num_embedding_dims])\n",
        "\n",
        "    dense_layer = tf.compat.v1.layers.Dense(\n",
        "        16, activation='relu')(\n",
        "            pooling_layer)\n",
        "\n",
        "    output_layer = tf.compat.v1.layers.Dense(\n",
        "        1, activation='sigmoid')(\n",
        "            dense_layer)\n",
        "\n",
        "  \n",
        "    return output_layer, dense_layer\n",
        "\n",
        "\n",
        "\n",
        "def embedding_fn(features, mode):\n",
        "\n",
        "  is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
        "  _, embedding = feed_forward_model(features, is_training)\n",
        "  return embedding\n",
        "\n",
        "\n",
        "def feed_forward_model_fn(features, labels, mode, params, config):\n",
        " \n",
        "\n",
        "  is_training = mode == tf.estimator.ModeKeys.TRAIN\n",
        "\n",
        "  probabilities, _ = feed_forward_model(features, is_training)\n",
        "  predictions = tf.round(probabilities)\n",
        "\n",
        "  if mode == tf.estimator.ModeKeys.PREDICT:\n",
        "    cross_entropy_loss = None\n",
        "    eval_metric_ops = None\n",
        "  else:\n",
        "   \n",
        "    probabilities = tf.reshape(probabilities, shape=[-1])\n",
        "    cross_entropy_loss = tf.compat.v1.keras.losses.binary_crossentropy(\n",
        "        labels, probabilities)\n",
        "    eval_metric_ops = {\n",
        "        'accuracy': tf.compat.v1.metrics.accuracy(labels, predictions)\n",
        "    }\n",
        "\n",
        "  if is_training:\n",
        "    global_step = tf.compat.v1.train.get_or_create_global_step()\n",
        "    train_op = build_train_op(cross_entropy_loss, global_step)\n",
        "  else:\n",
        "    train_op = None\n",
        "\n",
        "  return tf.estimator.EstimatorSpec(\n",
        "      mode=mode,\n",
        "      predictions={\n",
        "          'probabilities': probabilities,\n",
        "          'predictions': predictions\n",
        "      },\n",
        "      loss=cross_entropy_loss,\n",
        "      train_op=train_op,\n",
        "      eval_metric_ops=eval_metric_ops)\n",
        "\n",
        "\n",
        "def _get_raw_feature_spec(schema):\n",
        "  return schema_utils.schema_as_feature_spec(schema).feature_spec\n",
        "\n",
        "\n",
        "def _gzip_reader_fn(filenames):\n",
        "  return tf.data.TFRecordDataset(\n",
        "      filenames,\n",
        "      compression_type='GZIP')\n",
        "\n",
        "\n",
        "def _example_serving_receiver_fn(tf_transform_output, schema):\n",
        " \n",
        "  raw_feature_spec = _get_raw_feature_spec(schema)\n",
        "  raw_feature_spec.pop(LABEL_KEY)\n",
        "\n",
        "  raw_feature_spec.pop(ID_FEATURE_KEY)\n",
        "\n",
        "  raw_input_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(\n",
        "      raw_feature_spec, default_batch_size=None)\n",
        "  serving_input_receiver = raw_input_fn()\n",
        "\n",
        "  transformed_features = tf_transform_output.transform_raw_features(\n",
        "      serving_input_receiver.features)\n",
        "\n",
        " \n",
        "  transformed_features.pop(_transformed_name(LABEL_KEY))\n",
        "  return tf.estimator.export.ServingInputReceiver(\n",
        "      transformed_features, serving_input_receiver.receiver_tensors)\n",
        "\n",
        "\n",
        "def _eval_input_receiver_fn(tf_transform_output, schema):\n",
        " \n",
        "  raw_feature_spec = _get_raw_feature_spec(schema)\n",
        "\n",
        "  raw_feature_spec.pop(ID_FEATURE_KEY)\n",
        "\n",
        "  raw_input_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(\n",
        "      raw_feature_spec, default_batch_size=None)\n",
        "  serving_input_receiver = raw_input_fn()\n",
        "\n",
        "  transformed_features = tf_transform_output.transform_raw_features(\n",
        "      serving_input_receiver.features)\n",
        "\n",
        "  labels = transformed_features.pop(_transformed_name(LABEL_KEY))\n",
        "  return tfma.export.EvalInputReceiver(\n",
        "      features=transformed_features,\n",
        "      receiver_tensors=serving_input_receiver.receiver_tensors,\n",
        "      labels=labels)\n",
        "\n",
        "\n",
        "def _augment_feature_spec(feature_spec, num_neighbors):\n",
        " \n",
        "  for i in range(num_neighbors):\n",
        "    feature_spec['{}{}_{}'.format(NBR_FEATURE_PREFIX, i, 'id')] = \\\n",
        "        tf.io.VarLenFeature(dtype=tf.string)\n",
        "   \n",
        "    feature_spec['{}{}_{}'.format(NBR_FEATURE_PREFIX, i, 'text_xf')] = \\\n",
        "        tf.io.FixedLenFeature(shape=[HPARAMS.max_seq_length], dtype=tf.int64,\n",
        "                              default_value=tf.constant(0, dtype=tf.int64,\n",
        "                                                        shape=[HPARAMS.max_seq_length]))\n",
        "  \n",
        "  for i in range(num_neighbors):\n",
        "    feature_spec['{}{}{}'.format(NBR_FEATURE_PREFIX, i, NBR_WEIGHT_SUFFIX)] = \\\n",
        "        tf.io.FixedLenFeature(shape=[1], dtype=tf.float32, default_value=[0.0])\n",
        "\n",
        "  return feature_spec\n",
        "\n",
        "\n",
        "def _input_fn(filenames, tf_transform_output, is_training, batch_size=200):\n",
        " \n",
        "  transformed_feature_spec = (\n",
        "      tf_transform_output.transformed_feature_spec().copy())\n",
        "\n",
        " \n",
        "  if is_training:\n",
        "    transformed_feature_spec =_augment_feature_spec(transformed_feature_spec,\n",
        "                                                    HPARAMS.num_neighbors)\n",
        "\n",
        "  dataset = tf.data.experimental.make_batched_features_dataset(\n",
        "      filenames, batch_size, transformed_feature_spec, reader=_gzip_reader_fn)\n",
        "\n",
        "  transformed_features = tf.compat.v1.data.make_one_shot_iterator(\n",
        "      dataset).get_next()\n",
        "\n",
        "  return transformed_features, transformed_features.pop(\n",
        "      _transformed_name(LABEL_KEY))\n",
        "\n",
        "\n",
        "def trainer_fn(hparams, schema):\n",
        " \n",
        "  train_batch_size = 40\n",
        "  eval_batch_size = 40\n",
        "\n",
        "  tf_transform_output = tft.TFTransformOutput(hparams.transform_output)\n",
        "\n",
        "  train_input_fn = lambda: _input_fn(\n",
        "      hparams.train_files,\n",
        "      tf_transform_output,\n",
        "      is_training=True,\n",
        "      batch_size=train_batch_size)\n",
        "\n",
        "  eval_input_fn = lambda: _input_fn(\n",
        "      hparams.eval_files,\n",
        "      tf_transform_output,\n",
        "      is_training=False,\n",
        "      batch_size=eval_batch_size)\n",
        "\n",
        "  train_spec = tf.estimator.TrainSpec(\n",
        "      train_input_fn,\n",
        "      max_steps=hparams.train_steps)\n",
        "\n",
        "  serving_receiver_fn = lambda: _example_serving_receiver_fn(\n",
        "      tf_transform_output, schema)\n",
        "\n",
        "  exporter = tf.estimator.FinalExporter('imdb', serving_receiver_fn)\n",
        "  eval_spec = tf.estimator.EvalSpec(\n",
        "      eval_input_fn,\n",
        "      steps=hparams.eval_steps,\n",
        "      exporters=[exporter],\n",
        "      name='imdb-eval')\n",
        "\n",
        "  run_config = tf.estimator.RunConfig(\n",
        "      save_checkpoints_steps=999, keep_checkpoint_max=1)\n",
        "\n",
        "  run_config = run_config.replace(model_dir=hparams.serving_model_dir)\n",
        "\n",
        "  estimator = tf.estimator.Estimator(\n",
        "      model_fn=feed_forward_model_fn, config=run_config, params=HPARAMS)\n",
        "\n",
        "  graph_reg_config = nsl.configs.make_graph_reg_config(\n",
        "      max_neighbors=HPARAMS.num_neighbors,\n",
        "      multiplier=HPARAMS.graph_regularization_multiplier,\n",
        "      distance_type=HPARAMS.distance_type,\n",
        "      sum_over_axis=-1)\n",
        "\n",
        " \n",
        "  graph_nsl_estimator = nsl.estimator.add_graph_regularization(\n",
        "      estimator,\n",
        "      embedding_fn,\n",
        "      optimizer_fn=optimizer_fn,\n",
        "      graph_reg_config=graph_reg_config)\n",
        "\n",
        "  receiver_fn = lambda: _eval_input_receiver_fn(\n",
        "      tf_transform_output, schema)\n",
        "\n",
        "  return {\n",
        "      'estimator': graph_nsl_estimator,\n",
        "      'train_spec': train_spec,\n",
        "      'eval_spec': eval_spec,\n",
        "      'eval_input_receiver_fn': receiver_fn\n",
        "  }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pi2qAIg4EDWH"
      },
      "source": [
        "trainer = Trainer(\n",
        "    module_file=_trainer_module_file,\n",
        "    transformed_examples=graph_augmentation.outputs['augmented_examples'],\n",
        "    schema=schema_gen.outputs['schema'],\n",
        "    transform_graph=transform.outputs['transform_graph'],\n",
        "    train_args=trainer_pb2.TrainArgs(num_steps=10000),\n",
        "    eval_args=trainer_pb2.EvalArgs(num_steps=5000))\n",
        "context.run(trainer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCwlkeNsEPlj"
      },
      "source": [
        "train_uri = trainer.outputs['model'].get()[0].uri\n",
        "serving_model_path = os.path.join(train_uri, 'serving_model_dir')\n",
        "exported_model = tf.saved_model.load(serving_model_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXJNBraLEQi6"
      },
      "source": [
        "model_run_dir = trainer.outputs['model_run'].get()[0].uri\n",
        "\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir {model_run_dir}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xODZUqEETtH"
      },
      "source": [
        "import absl\n",
        "import pandas as pd\n",
        "from tfx.components import CsvExampleGen\n",
        "_tfx_root = tfx.__path__[0]\n",
        "_taxi_root = os.path.join(_tfx_root, 'examples/iris_pca_example')\n",
        "\n",
        "absl.logging.set_verbosity(absl.logging.INFO)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CQB_PgkEXqr"
      },
      "source": [
        "_data_root = tempfile.mkdtemp(prefix='tfx-data')\n",
        "DATA_PATH = 'https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/penguin/data/penguins_processed.csv'\n",
        "with open(os.path.join(_data_root, 'data.csv'), 'wb') as f:\n",
        "    contents = urllib.request.urlopen(DATA_PATH).read()\n",
        "    f.write(contents)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGq9x-jQEc7a"
      },
      "source": [
        "data_test1 = pd.read_csv(DATA_PATH)\n",
        "data_test1.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kb4jhWY6Eia6"
      },
      "source": [
        "data_test1.info"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcB3n2HWEldN"
      },
      "source": [
        "context = InteractiveContext()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfnBPtlOEpMW"
      },
      "source": [
        "artifact = example_gen.outputs['examples'].get()[0]\n",
        "print(artifact.split_names, artifact.uri)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pd0A7tvrEtDW"
      },
      "source": [
        "examples = external_input(_data_root)\n",
        "\n",
        "example_gen = CsvExampleGen(input=examples)\n",
        "context.run(example_gen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d73Y6FhpEw63"
      },
      "source": [
        "train_uri = os.path.join(example_gen.outputs['examples'].get()[0].uri, 'train')\n",
        "\n",
        "tfrecord_filenames = [os.path.join(train_uri, name)\n",
        "                      for name in os.listdir(train_uri)]\n",
        "\n",
        "dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")\n",
        "\n",
        "for tfrecord in dataset.take(3):\n",
        "  serialized_example = tfrecord.numpy()\n",
        "  example = tf.train.Example()\n",
        "  example.ParseFromString(serialized_example)\n",
        "  pp.pprint(example)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E05jfl4oE4cN"
      },
      "source": [
        "statistics_gen = StatisticsGen(\n",
        "    examples=example_gen.outputs['examples'])\n",
        "context.run(statistics_gen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p97AQDSBE8ET"
      },
      "source": [
        "schema_gen = SchemaGen(\n",
        "    statistics=statistics_gen.outputs['statistics'],\n",
        "    infer_feature_shape=False)\n",
        "context.run(schema_gen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0rJdzFzE_XX"
      },
      "source": [
        "context.show(statistics_gen.outputs['statistics'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFOUm5qsFDB9"
      },
      "source": [
        "context.show(schema_gen.outputs['schema'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LX_duS0FIPq"
      },
      "source": [
        "example_validator = ExampleValidator(\n",
        "    statistics=statistics_gen.outputs['statistics'],\n",
        "    schema=schema_gen.outputs['schema'])\n",
        "context.run(example_validator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8YpFGFSFI3J"
      },
      "source": [
        "_iris_pca_transform_module_file = 'iris_pca_transform.py'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yTCjbj9FOaJ"
      },
      "source": [
        "%%writefile {_iris_pca_transform_module_file}\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_transform as tft\n",
        "\n",
        "LABEL_KEY = 'variety'\n",
        "\n",
        "def _fill_in_missing(x):\n",
        "   \n",
        "    default_value = '' if x.dtype == tf.string else 0\n",
        "    return tf.sparse.to_dense(tf.SparseTensor(x.indices, x.values, [x.dense_shape[0], 1]), \n",
        "                              default_value)\n",
        "      \n",
        "def preprocessing_fn(inputs):\n",
        "    features = []\n",
        "    outputs = {\n",
        "        LABEL_KEY: _fill_in_missing(inputs[LABEL_KEY])\n",
        "    }\n",
        "    \n",
        "    for feature_name, feature_tensor in inputs.items():\n",
        "        if feature_name != LABEL_KEY:\n",
        "            features.append(tft.scale_to_z_score( \n",
        "                _fill_in_missing(feature_tensor)  \n",
        "            ))\n",
        "\n",
        "    \n",
        "    feature_matrix = tf.concat(features, axis=1)  \n",
        "    \n",
        "    \n",
        "    orthonormal_vectors = tft.pca(feature_matrix, output_dim=2, dtype=tf.float32)\n",
        "    \n",
        "    \n",
        "    pca_examples = tf.linalg.matmul(feature_matrix, orthonormal_vectors)\n",
        "    \n",
        "    \n",
        "    pca_examples = tf.unstack(pca_examples, axis=1)\n",
        "    outputs['Principal Component 1'] = pca_examples[0]\n",
        "    outputs['Principal Component 2'] = pca_examples[1]\n",
        "      \n",
        "    return outputs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIMJTAKvFUzB"
      },
      "source": [
        "transform = Transform(\n",
        "    examples=example_gen.outputs['examples'],\n",
        "    schema=schema_gen.outputs['schema'],\n",
        "    module_file=os.path.abspath(_iris_pca_transform_module_file))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkjtl2NjFXn1"
      },
      "source": [
        "transform.outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGKxka4hFcA3"
      },
      "source": [
        "_iris_trainer_module_file = 'iris_trainer.py'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gs-XWf9xFlg2"
      },
      "source": [
        "%%writefile {_iris_trainer_module_file}\n",
        "\n",
        "from typing import List, Text\n",
        "\n",
        "import os\n",
        "import absl\n",
        "import datetime\n",
        "import tensorflow as tf\n",
        "import tensorflow_transform as tft\n",
        "\n",
        "from tfx.components.trainer.executor import TrainerFnArgs\n",
        "from tfx.components.trainer.fn_args_utils import DataAccessor\n",
        "from tfx_bsl.tfxio import dataset_options\n",
        "\n",
        "import taxi_constants\n",
        "\n",
        "_DENSE_FLOAT_FEATURE_KEYS = taxi_constants.DENSE_FLOAT_FEATURE_KEYS\n",
        "_VOCAB_FEATURE_KEYS = taxi_constants.VOCAB_FEATURE_KEYS\n",
        "_VOCAB_SIZE = taxi_constants.VOCAB_SIZE\n",
        "_OOV_SIZE = taxi_constants.OOV_SIZE\n",
        "_FEATURE_BUCKET_COUNT = taxi_constants.FEATURE_BUCKET_COUNT\n",
        "_BUCKET_FEATURE_KEYS = taxi_constants.BUCKET_FEATURE_KEYS\n",
        "_CATEGORICAL_FEATURE_KEYS = taxi_constants.CATEGORICAL_FEATURE_KEYS\n",
        "_MAX_CATEGORICAL_FEATURE_VALUES = taxi_constants.MAX_CATEGORICAL_FEATURE_VALUES\n",
        "_LABEL_KEY = taxi_constants.LABEL_KEY\n",
        "_transformed_name = taxi_constants.transformed_name\n",
        "\n",
        "\n",
        "def _transformed_names(keys):\n",
        "  return [_transformed_name(key) for key in keys]\n",
        "\n",
        "\n",
        "def _get_serve_tf_examples_fn(model, tf_transform_output):\n",
        " \n",
        "\n",
        "  model.tft_layer = tf_transform_output.transform_features_layer()\n",
        "\n",
        "  @tf.function\n",
        "  def serve_tf_examples_fn(serialized_tf_examples):\n",
        "   \n",
        "    feature_spec = tf_transform_output.raw_feature_spec()\n",
        "    feature_spec.pop(_LABEL_KEY)\n",
        "    parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)\n",
        "    transformed_features = model.tft_layer(parsed_features)\n",
        "    return model(transformed_features)\n",
        "\n",
        "  return serve_tf_examples_fn\n",
        "\n",
        "\n",
        "def _input_fn(file_pattern: List[Text],\n",
        "              data_accessor: DataAccessor,\n",
        "              tf_transform_output: tft.TFTransformOutput,\n",
        " \n",
        "  return data_accessor.tf_dataset_factory(\n",
        "      file_pattern,\n",
        "      dataset_options.TensorFlowDatasetOptions(\n",
        "          batch_size=batch_size, label_key=_transformed_name(_LABEL_KEY)),\n",
        "      tf_transform_output.transformed_metadata.schema)\n",
        "\n",
        "\n",
        "def _build_keras_model(hidden_units: List[int] = None) -> tf.keras.Model:\n",
        " \n",
        "  real_valued_columns = [\n",
        "      tf.feature_column.numeric_column(key, shape=())\n",
        "      for key in _transformed_names(_DENSE_FLOAT_FEATURE_KEYS)\n",
        "  ]\n",
        "  categorical_columns = [\n",
        "      tf.feature_column.categorical_column_with_identity(\n",
        "          key, num_buckets=_VOCAB_SIZE + _OOV_SIZE, default_value=0)\n",
        "      for key in _transformed_names(_VOCAB_FEATURE_KEYS)\n",
        "  ]\n",
        "  categorical_columns += [\n",
        "      tf.feature_column.categorical_column_with_identity(\n",
        "          key, num_buckets=_FEATURE_BUCKET_COUNT, default_value=0)\n",
        "      for key in _transformed_names(_BUCKET_FEATURE_KEYS)\n",
        "  ]\n",
        "  categorical_columns += [\n",
        "      tf.feature_column.categorical_column_with_identity(  # pylint: disable=g-complex-comprehension\n",
        "          key,\n",
        "          num_buckets=num_buckets,\n",
        "          default_value=0) for key, num_buckets in zip(\n",
        "              _transformed_names(_CATEGORICAL_FEATURE_KEYS),\n",
        "              _MAX_CATEGORICAL_FEATURE_VALUES)\n",
        "  ]\n",
        "  indicator_column = [\n",
        "      tf.feature_column.indicator_column(categorical_column)\n",
        "      for categorical_column in categorical_columns\n",
        "  ]\n",
        "\n",
        "  model = _wide_and_deep_classifier(\n",
        "    \n",
        "      wide_columns=indicator_column,\n",
        "      deep_columns=real_valued_columns,\n",
        "      dnn_hidden_units=hidden_units or [100, 70, 50, 25])\n",
        "  return model\n",
        "\n",
        "\n",
        "def _wide_and_deep_classifier(wide_columns, deep_columns, dnn_hidden_units):\n",
        "  \n",
        "  input_layers = {\n",
        "      colname: tf.keras.layers.Input(name=colname, shape=(), dtype=tf.float32)\n",
        "      for colname in _transformed_names(_DENSE_FLOAT_FEATURE_KEYS)\n",
        "  }\n",
        "  input_layers.update({\n",
        "      colname: tf.keras.layers.Input(name=colname, shape=(), dtype='int32')\n",
        "      for colname in _transformed_names(_VOCAB_FEATURE_KEYS)\n",
        "  })\n",
        "  input_layers.update({\n",
        "      colname: tf.keras.layers.Input(name=colname, shape=(), dtype='int32')\n",
        "      for colname in _transformed_names(_BUCKET_FEATURE_KEYS)\n",
        "  })\n",
        "  input_layers.update({\n",
        "      colname: tf.keras.layers.Input(name=colname, shape=(), dtype='int32')\n",
        "      for colname in _transformed_names(_CATEGORICAL_FEATURE_KEYS)\n",
        "  })\n",
        "\n",
        "  deep = tf.keras.layers.DenseFeatures(deep_columns)(input_layers)\n",
        "  for numnodes in dnn_hidden_units:\n",
        "    deep = tf.keras.layers.Dense(numnodes)(deep)\n",
        "  wide = tf.keras.layers.DenseFeatures(wide_columns)(input_layers)\n",
        "\n",
        "  output = tf.keras.layers.Dense(\n",
        "      1, activation='sigmoid')(\n",
        "          tf.keras.layers.concatenate([deep, wide]))\n",
        "\n",
        "  model = tf.keras.Model(input_layers, output)\n",
        "  model.compile(\n",
        "      loss='binary_crossentropy',\n",
        "      optimizer=tf.keras.optimizers.Adam(lr=0.001),\n",
        "      metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
        "  model.summary(print_fn=absl.logging.info)\n",
        "  return model\n",
        "\n",
        "\n",
        "def run_fn(fn_args: TrainerFnArgs):\n",
        "  \n",
        "  first_dnn_layer_size = 100\n",
        "  num_dnn_layers = 4\n",
        "  dnn_decay_factor = 0.7\n",
        "\n",
        "  tf_transform_output = tft.TFTransformOutput(fn_args.transform_output)\n",
        "\n",
        "  train_dataset = _input_fn(fn_args.train_files, fn_args.data_accessor, \n",
        "                            tf_transform_output, 40)\n",
        "  eval_dataset = _input_fn(fn_args.eval_files, fn_args.data_accessor, \n",
        "                           tf_transform_output, 40)\n",
        "\n",
        "  model = _build_keras_model(\n",
        "      hidden_units=[\n",
        "          max(2, int(first_dnn_layer_size * dnn_decay_factor**i))\n",
        "          for i in range(num_dnn_layers)\n",
        "      ])\n",
        "\n",
        "  tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
        "      log_dir=fn_args.model_run_dir, update_freq='batch')\n",
        "  model.fit(\n",
        "      train_dataset,\n",
        "      steps_per_epoch=fn_args.train_steps,\n",
        "      validation_data=eval_dataset,\n",
        "      validation_steps=fn_args.eval_steps,\n",
        "      callbacks=[tensorboard_callback])\n",
        "\n",
        "  signatures = {\n",
        "      'serving_default':\n",
        "          _get_serve_tf_examples_fn(model,\n",
        "                                    tf_transform_output).get_concrete_function(\n",
        "                                        tf.TensorSpec(\n",
        "                                            shape=[None],\n",
        "                                            dtype=tf.string,\n",
        "                                            name='examples')),\n",
        "  }\n",
        "  model.save(fn_args.serving_model_dir, save_format='tf', signatures=signatures)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}